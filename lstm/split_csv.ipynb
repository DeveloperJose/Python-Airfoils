{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1800102/2495241499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# https://blog.nelsonliu.me/2016/07/30/progress-bars-for-python-file-reading-with-tqdm/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programming/git-python-airfoils/lstm/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import mmap\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import data\n",
    "\n",
    "# https://blog.nelsonliu.me/2016/07/30/progress-bars-for-python-file-reading-with-tqdm/\n",
    "# This is used for the progress bar so we can keep track of the progress\n",
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    fp.close()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumped from timestep 0 to timestep 342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jperez/miniconda3/envs/keras/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: Could not cast to float64, falling back to object. This behavior is deprecated. In a future version, when a dtype is passed to 'DataFrame', either all columns will be cast to that dtype, or a TypeError will be raised\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "Splitting big CSV file into individual timesteps: 100%|█████████▉| 5264119/5264490 [02:16<00:00, 38673.39it/s, Timestep 371]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/home/jperez/data/sled/250_part2.csv'\n",
    "output_dir = '/home/jperez/data/sled250/'\n",
    "columns_to_save = data.COLUMNS_RAW\n",
    "\n",
    "# Overall dataset statistics\n",
    "row_counts = []\n",
    "header_counts = []\n",
    "plane_timesteps = []\n",
    "\n",
    "with open(input_path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    p_bar = tqdm(desc='Splitting big CSV file into individual timesteps', postfix='Timestep 0', total=get_num_lines(input_path))\n",
    "\n",
    "    data_header = []\n",
    "    data_rows = []\n",
    "\n",
    "    prev_timestep = 0\n",
    "\n",
    "    for row in reader:\n",
    "        # Update progress bar\n",
    "        p_bar.update(1)\n",
    "        # Skip empty rows\n",
    "        if len(row) == 0:\n",
    "            continue\n",
    "        # Look for the start of a new file\n",
    "        elif row[0] == '[Name]':\n",
    "            # Check if we have data to save\n",
    "            if len(data_rows) > 0:\n",
    "                # Save some dataset statistics to analyze later\n",
    "                header_counts.append(len(data_header))\n",
    "                row_counts.append(len(data_rows))\n",
    "\n",
    "                # Create dataframe from the data and save it\n",
    "                df = pd.DataFrame(data=data_rows, columns=data_header, dtype=np.float64)\n",
    "                df = df.rename(columns=lambda col: col.strip())\n",
    "                df['Timestep'] = current_timestep\n",
    "\n",
    "                # Save CSV if you want\n",
    "                # df.to_csv(os.path.join(output_dir, f'{prev_timestep}.csv'), index=False, columns=columns_to_save)\n",
    "                \n",
    "                # Saving to NP file format is faster\n",
    "                np.save(os.path.join(output_dir, f'{prev_timestep}'), df[columns_to_save].to_numpy().astype(np.float64))\n",
    "\n",
    "                # Empty data lists for next CSV file\n",
    "                data_header = []\n",
    "                data_rows = []\n",
    "        # Get the data header for this file\n",
    "        elif row[0] == '[Data]':\n",
    "            data_header = next(reader)\n",
    "        # Check what timestep we are on\n",
    "        elif row[0][:5] == 'Plane':\n",
    "            # This will get 00001.250 and split it\n",
    "            ts_split = row[0][-8:].split('.')\n",
    "            # From the split, get the timestep as an integer\n",
    "            try: \n",
    "                current_timestep = int(ts_split[0])\n",
    "            except:\n",
    "                try:\n",
    "                    current_timestep = int(row[0][-4:])\n",
    "                except:\n",
    "                    print(f'Could not process timestep in row={row}')\n",
    "                    current_timestep = None\n",
    "\n",
    "            if current_timestep:\n",
    "                # Compare against previous timestep to see if we jumped more than 1\n",
    "                if current_timestep - prev_timestep > 1:\n",
    "                    print(f'Jumped from timestep {prev_timestep} to timestep {current_timestep}')\n",
    "\n",
    "                # Update variables and progress bar\n",
    "                prev_timestep = current_timestep\n",
    "                plane_timesteps.append(current_timestep)\n",
    "                p_bar.postfix = f'Timestep {current_timestep}'\n",
    "        # Otherwise just collect data rows\n",
    "        else:\n",
    "            data_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Plane 1 in Case FFF']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that all files have the same number of rows [14184]\n",
      "Check that all files have the same header length [37]\n",
      "From 342 to 742\n"
     ]
    }
   ],
   "source": [
    "print('Check that all files have the same number of rows', np.unique(row_counts))\n",
    "print('Check that all files have the same header length', np.unique(header_counts))\n",
    "print(f'From {plane_timesteps[0]} to {plane_timesteps[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14184, 3) (14184, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.46944695e-17,  2.50000000e-01,  2.01000000e+02],\n",
       "       [ 4.29344060e-17,  2.75000006e-01,  2.01000000e+02],\n",
       "       [-2.50000004e-02,  2.75000006e-01,  2.01000000e+02],\n",
       "       [-2.50000004e-02,  2.50000000e-01,  2.01000000e+02],\n",
       "       [ 3.30627466e-17,  2.24999994e-01,  2.01000000e+02]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = data.read_np('/home/jperez/data/sled250/201.npy', ['X', 'Y', 'T'], ['Vu', 'Vv'], use_2D=False, scaler=None)\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "X[:5]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbe1d1abaf7dd2ded8faa7720bf0e1d4f69b9081fc349f0d9bde228c06049906"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('keras': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
