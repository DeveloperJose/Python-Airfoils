{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previous scaler\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import joblib\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from lstm_v3 import StandardScaler3D, read_np\n",
    "\n",
    "INPUTS = [0, 1, 2, 3, 5]\n",
    "OUTPUTS = [3]\n",
    "\n",
    "LOOKBACK = 10\n",
    "BATCH_SIZE = 5024\n",
    "CONVERT_TO_2D = False\n",
    "\n",
    "#%% Scaler. Recreating takes around 3 minutes\n",
    "SCALER_PATH = f'scaler_{INPUTS}_2D={CONVERT_TO_2D}.pkl'\n",
    "SCALER_CREATION_DIRS = ['/home/jperez/data/sled250']\n",
    "\n",
    "if os.path.exists(SCALER_PATH):\n",
    "    print('Loading previous scaler')\n",
    "    SCALER = joblib.load(SCALER_PATH)\n",
    "else:\n",
    "    print('Recreating scaler')\n",
    "    if CONVERT_TO_2D:\n",
    "        SCALER = StandardScaler3D()\n",
    "    else:\n",
    "        SCALER = StandardScaler()\n",
    "    for creation_dir in SCALER_CREATION_DIRS:\n",
    "        for filepath in tqdm(list(Path(creation_dir).glob('*.npy'))):\n",
    "            X, Y = read_np(filepath, None)\n",
    "            SCALER.partial_fit(X)\n",
    "    joblib.dump(SCALER, SCALER_PATH)\n",
    "\n",
    "class SledDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, data_dir, batch_size, lookback, shuffle, start, end, step=1):\n",
    "        print(f'Loading dataset {data_dir} from t={start} to t={end} with 2D={CONVERT_TO_2D}')\n",
    "\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.batch_size = batch_size\n",
    "        self.lookback = lookback\n",
    "        self.shuffle = shuffle\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.step = step\n",
    "        \n",
    "        # Check if we have a serialized version of the data, if not, generate it\n",
    "        # n_count = ((end-start)//step)+1\n",
    "        data_path = f'{self.data_dir.stem}_{start}_{end}_in_{INPUTS}_out_{OUTPUTS}_2d_{CONVERT_TO_2D}.npz'\n",
    "        if os.path.exists(data_path):\n",
    "            csv_data = np.load(data_path)\n",
    "            self.x_data = csv_data['X']\n",
    "            self.y_data = csv_data['Y']\n",
    "        else:\n",
    "            self.x_data = []\n",
    "            self.y_data = []\n",
    "\n",
    "            for timestep in tqdm(range(start, end, step)):\n",
    "                X, Y = self.__read_np__(timestep)\n",
    "                self.x_data.append(X)\n",
    "                self.y_data.append(Y)\n",
    "            self.x_data = np.array(self.x_data)\n",
    "            self.y_data = np.array(self.y_data)\n",
    "            np.savez(data_path, X=self.x_data, Y=self.y_data)\n",
    "        print('Debug: X=', self.x_data.shape, 'Y=', self.y_data.shape)\n",
    "\n",
    "        # Some sanity checks\n",
    "        if not CONVERT_TO_2D:\n",
    "            assert self.x_data.shape[1] == self.y_data.shape[1], 'x_data and y_data have a shape mismatch in the number of CSV rows'\n",
    "            # assert self.x_data.shape[1] == N_CSV_ROWS, f'N_CSV_ROWS does not match {self.x_data.shape[1]}'\n",
    "\n",
    "        # Helpful variables\n",
    "        # 1D: [timestep, inputs, 1]\n",
    "        # 2D: [timestep, height, width, inputs, 1]\n",
    "        self.n_timesteps = self.x_data.shape[0]\n",
    "\n",
    "        # Generate a list of the valid timesteps for batches\n",
    "        self.list_timesteps = np.arange(lookback, self.n_timesteps)\n",
    "        # self.list_rows = np.arange(N_CSV_ROWS)\n",
    "        self.list_rows = np.arange(self.x_data.shape[1])\n",
    "        self.list_IDs = [(t, r) for t in self.list_timesteps for r in self.list_rows]\n",
    "        # self.list_IDs = [x for x in itertools.product(self.list_timesteps, self.list_rows)]\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # return int(np.floor(len(self.list_timesteps) / self.batch_size))\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_idxs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        batch_pairs = [self.list_IDs[k] for k in batch_idxs]\n",
    "\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for (timestep, row) in batch_pairs:\n",
    "            if CONVERT_TO_2D:\n",
    "                X = self.x_data[timestep-self.lookback:timestep, :, :]\n",
    "                Y = self.y_data[timestep, :, :]\n",
    "            else:\n",
    "                X = self.x_data[timestep-self.lookback:timestep, row]\n",
    "                Y = self.y_data[timestep, row]\n",
    "\n",
    "            batch_x.append(X)\n",
    "            batch_y.append(Y)\n",
    "\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    # def __read_csv__(self, timestep):\n",
    "    #     filename = os.path.join(self.data_dir, f'export{timestep}.csv')\n",
    "    #     return read_csv(filename, SCALER)\n",
    "\n",
    "    def __read_np__(self, timestep):\n",
    "        filename = os.path.join(self.data_dir, f'{timestep}.npy')\n",
    "        return read_np(filename, SCALER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_EXPERIMENT_N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('LSTM_v3_exp17.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset /home/jperez/data/sled250 from t=510 to t=639 with 2D=False\n",
      "Debug: X= (129, 14184, 5) Y= (129, 14184, 1)\n",
      "335/335 [==============================] - 10s 25ms/step - loss: 1.4799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.47993004322052"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_generator = SledDataGenerator('/home/jperez/data/sled250', batch_size=BATCH_SIZE, lookback=LOOKBACK, shuffle=True, start=510, end=638+1)\n",
    "model.evaluate(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset /home/jperez/data/sled255 from t=19 to t=761 with 2D=False\n",
      "Debug: X= (742, 14184, 5) Y= (742, 14184, 1)\n",
      "2066/2066 [==============================] - 64s 31ms/step - loss: 27168.7969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27168.796875"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_generator = SledDataGenerator('/home/jperez/data/sled255', batch_size=BATCH_SIZE, lookback=LOOKBACK, shuffle=True, start=19, end=760+1)\n",
    "model.evaluate(val_generator)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbe1d1abaf7dd2ded8faa7720bf0e1d4f69b9081fc349f0d9bde228c06049906"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('keras': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
